{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True) # True, loads the data in a 2-tuple structure[input,target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=3.0.1,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess the data, first we split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples # we take validation samples from the training by tf operations\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64) # as we're not sure if the validation will be integer, so we make it integer anyway by this formula\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image, tf.float32) # as the images number between 0-255 , we divide it by 255 to get range(0,1)\n",
    "    image /=255. #the dot means that we want the image as float\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 1000 # this attribute to shuffle the data 1000 samples each time, if its 1, then no shuffling, why to shuffle? \n",
    "# because we want the model learn better with no easy reading, usually if the model is not shuffled, the data will be sorted and easy to guess.\n",
    "# so shuffling optmizes the computational power\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples) # validation dataset created\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples) #means that don't mix validation with the training data\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE) # this method combines the consecutive elements of the dataset into batchs each = 100\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 782\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)), #flat matrices into column vectors \n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #h1\n",
    "                            #tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), #h2\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #h3\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choosing the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model, here we fit the model we built and see if its actually well made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens inside epoch?\n",
    "1. at the beginning of each epoch, the training loss will be set to zero\n",
    "2. the algorithm will iterate over a preset number of batches, all from train_data\n",
    "3. the weights and biases will be updated as many times as there are batches\n",
    "4. we will get a value of the loss func, indicating how the training is going\n",
    "5. we will also see the training accuracy\n",
    "6. at the end , the algorithm will forward propagate the whole validation set\n",
    "when reaching max epochs, train will be over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 3s - loss: 0.0258 - accuracy: 0.9920 - val_loss: 0.0905 - val_accuracy: 0.9748\n",
      "Epoch 2/10\n",
      "540/540 - 3s - loss: 0.0228 - accuracy: 0.9926 - val_loss: 0.0771 - val_accuracy: 0.9793\n",
      "Epoch 3/10\n",
      "540/540 - 3s - loss: 0.0195 - accuracy: 0.9933 - val_loss: 0.1120 - val_accuracy: 0.9697\n",
      "Epoch 4/10\n",
      "540/540 - 3s - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.0763 - val_accuracy: 0.9802\n",
      "Epoch 5/10\n",
      "540/540 - 3s - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.1002 - val_accuracy: 0.9772\n",
      "Epoch 6/10\n",
      "540/540 - 3s - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.0857 - val_accuracy: 0.9797\n",
      "Epoch 7/10\n",
      "540/540 - 3s - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.0875 - val_accuracy: 0.9798\n",
      "Epoch 8/10\n",
      "540/540 - 3s - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0820 - val_accuracy: 0.9813\n",
      "Epoch 9/10\n",
      "540/540 - 3s - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.0830 - val_accuracy: 0.9825\n",
      "Epoch 10/10\n",
      "540/540 - 3s - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.1019 - val_accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x213643e6148>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "model.fit(train_data, epochs =NUM_EPOCHS, validation_data=(validation_inputs,validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 997us/step - loss: 0.1108 - accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.11. Test accuracy: 97.99%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy *100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
